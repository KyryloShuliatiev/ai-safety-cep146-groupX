Hello everyone.
Today I will talk about AI Safety.
AI means artificial intelligence.
AI is getting very strong and very smart.

Because AI is growing fast, many people worry about safety.
Governments and big companies are making new rules to keep AI safe.

Let me explain the main problems.

First problem: fake videos and fake pictures.
AI can make videos and photos that look real, but they are not real.
These “deepfakes” can trick people and spread lies.

Second problem: dangerous information.
Some AI systems can give instructions about chemicals or other harmful things.
So companies block these dangerous questions.

Third problem: AI agents.
These AIs can do tasks by themselves.
Sometimes they do things that people did not expect.
This can be risky.

Fourth problem: wrong answers.
AI can speak very confidently, but the answer can be totally wrong.
This is bad for doctors, money experts, or lawyers.

Now, what are companies doing to make AI safer?

Big companies like OpenAI, Google, Meta, and Anthropic are working together.
They want to make sure AI is safe.

They use red-teaming.
This means experts try to make the AI fail, to find problems early.

They also use filters.
These filters stop the AI from answering dangerous questions.

They are working on watermarks.
Watermarks help people know if something was made by AI.

There are also safety checks.
Before a new AI model is released, companies write reports about safety.

Countries are also working together.
At the AI Safety Summit in the UK, many nations agreed to share ideas and rules for safe AI.

So why is AI Safety important?

AI is changing hospitals, schools, business, politics — everything.
If AI becomes too powerful without rules, it can cause big problems.

So here is our final question:

Who should control powerful AI?
Governments?
Tech companies?
Or a special group of independent experts?

Thank you for listening.
