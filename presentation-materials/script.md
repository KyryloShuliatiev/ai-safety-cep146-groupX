Hello everyone.
Today I will be presenting our project on AI Safety, a topic that has become extremely important as artificial intelligence continues to advance at a rapid pace.

In the last two years, AI systems have grown so powerful that governments and major technology companies around the world have started introducing new regulations, safety frameworks, and international agreements.
The goal of AI Safety is to make sure that AI systems remain safe, predictable, and beneficial — not harmful.

Let’s look at the key risks that experts are most concerned about.

The first major risk is misinformation and deepfakes.
Modern AI can produce fake videos, audio, and images that look completely real. This can influence elections, damage reputations, and spread false information extremely quickly.

The second risk involves biological threats.
Advanced AI models are sometimes capable of generating harmful instructions related to dangerous chemicals or biological agents. Because of this, companies now limit certain “high-risk” prompts.

The third risk is autonomous AI agents.
These systems can carry out tasks on their own, and sometimes they behave in unexpected ways. This raises concerns about AI taking actions that users did not intend or understand.

The fourth risk is hallucination.
AI can confidently provide information that is completely incorrect. In areas like medicine, finance, or legal advice, such mistakes can cause serious harm.

Now let’s talk about what companies are actually doing to reduce these risks.

Major organizations like OpenAI, Google, Meta, and Anthropic have signed global AI safety agreements and committed to stronger safety practices.

One important practice is red-teaming — teams of experts deliberately try to “break” the model or cause unsafe outputs to identify weaknesses.

Another practice is content filtering and access restrictions.
AI models now block prompts related to weapons, election interference, chemical threats, and other dangerous topics.

Companies are also working on watermarking, which is a technique to mark AI-generated content so people can easily identify whether something was created by a machine.

There are also independent evaluations, where organizations publish safety reports before releasing new models. This helps ensure transparency and builds public trust.

Finally, global cooperation is increasing.
During the AI Safety Summit in the United Kingdom, countries agreed to collaborate on shared safety standards and research.

So why does all of this matter?

AI is already changing healthcare, education, businesses, politics, and global security.
If these systems continue to grow more powerful without proper oversight, the social and economic consequences could be significant.

And this leads us to our final discussion question — the one we want our audience to think about:

Who should control and regulate advanced AI systems?
Governments?
Tech companies?
Or independent international organizations?

Thank you for listening.
